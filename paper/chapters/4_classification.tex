\section{Classification}


%Ideen:
%\begin{enumerate}
%\item Supervised vs. unsupervised
%\item Training Set
%\item FeatureVector
%\item N-Grams
%\end{enumerate}
%

Sentiment analysis of twitter data, or more general, the idea of extracting sentiment and opinions from pieces of text is based on the more general principle of \emph{classification} \cite{Pang2002}. The broad aim of classification is to assign given textual units to a set of classes or categories or the apply some kind of regression or ranking. Sentiment analysis is a specialization of this approach which aims at assigning sentiment values to documents. One application might involve to classify an opinionated text by assigning one of two opposing sentiment polarities, i.e. classifying it as either \emph{positive} or \emph{negative}. This classification task is also referred to as \emph{binary classification} or \emph{sentiment polarity classification} \cite{Pang2002}. But in general, the input to the sentiment classification process is not strictly opinionated which makes this task challenging. Therefore this kind of binary classification might not always be applicable. Different approaches that allow for a more fine grained classification might be appropriate, for example, based on a multi-point scale that allows for more than just two sentiment classes.

The characteristics of twitter messages introduce further challenges. Due to the structure of these messages, sentiment analysis of tweets is different from analyzing conventional texts, such as review documents, in various ways. The length of at most 140 characters and the rather informal spelling style pose problems that have to be considered carefully when analyzing the data. When preprocessing the texts these aspects already need to be handled so that the actual classification process is supplied with information that is considered useful for the analysis phase. After this step it is necessary to collect the relevant information as so called \emph{features} and organize them into a \emph{feature vector}.

\subsection{Features}
Since twitter messages are composed of an arbitrary number of words, classification makes it necessary to extract exactly those words which preferably describes the characteristics of such tweets. The extraction of those words by the use of various word selecting techniques is called a  feature vector. These word selecting techniques are a fundamental process for providing informations to the classification methods, thus an extensive body of different word selecting algorithms exist. An overview is provided by Pang et al. \autocite{Pang2008} which can be summarized as follows:\\
\emph{Term Presence} is a word selecting technique where the feature vector consists of a binary value, $0$ for "term is not present" and $1$ for "term is present".\\
A similar approach is to count the number of occurred words in a message and make use of the numeric value as a feature, which is called \emph{Term Frequency}.\\
\emph{Term-based Features Beyond Term Unigrams} describes the encoding of position informations within a textual unit (e.g., at the end of the document)  into the feature vectors. This technique is based on the idea that the position of textual units can have influence on the importance of these.\\
The idea of \emph{Part-of-Speech} (POS) informations is to make use of labeled words within the tweet by diversifying them into categories of words. As soon as the words are labeled, they can be filtered (f.e. by adjectives and verbs only which can be a strong indicator for sentiment) and the result can be applied to the feature vectors.\\

\subsection{Methods}

For the actual classification process extent literature distinguishes two general types of techniques, namely \emph{machine learning} and \emph{semantic orientation} \cite{Ye20096527}. The machine learning approach is also referred to as \emph{supervised learning}, since corresponding techniques generally require supervised training phases. Accordingly, semantic orientation is also called \emph{unsupervised learning} because such a training phase is not necessary for the process to work. The most prominent machine learning methods comprise \emph{support vector machines} and the \emph{Na\"ive Bayes} classifier.

\subsubsection*{Na\"ive Bayes}

This technique is based on a simple stochastic model that for a given document $d$ tries to find the most likely class $c^* = \mathrm{arg max}_c P(c \vert d)$ where $c$ stands for one out of a number of possible classes and $P(c \vert d)$ denotes the conditional probability that document $d$ can be assigned to $c$ \cite{Pang2002}. For example, in sentiment analysis $c^*$ would denote the sentiment value that could be assigned to document $d$. $c^*$ is computed by applying Bayes' rule,

\begin{equation*}
P(c \vert d) = \frac{P(c)P(d \vert c)}{P(d)}
\end{equation*}

for each possible sentiment value $c$. $P(d \vert c)$ has to be estimated and therefore it is presumed that there exists a predefined set of features $\lbrace f_1, \ldots , f_m \rbrace$ which are assumed to be conditionally independent. Based on this estimation the actual rule that can be used for classification is derived from Bayes' rule:

\begin{equation*}
P_{\mathrm{NB}}(c \vert d) := \frac{P(c)(\prod^{m}_{i=1}P(f_i \vert c)^{n_i(d)})}{P(d)}.
\end{equation*}

Here, $n_i(d)$ designates the number of occurrences of feature $f_i$ in document $d$.

\subsubsection*{Support Vector Machines}

In contrast to the above method, support vector machines (SVMs) do not use a probabilistic model. Instead the aim of this technique is to find a so called \emph{hyperplane}, which is represented by a vector $\vec{\omega}$. Let $\vec{d} := (n_1(d), n_2(d),\ldots , n_m(d))$ be the \emph{document vector} of document $d$ where, like above, $n_i(d)$ represents the number of occurrences of feature $f_i$ in $d$. Then, in the case of a binary classification problem, $\vec{\omega}$ separates the document vectors in one class from the vectors in the other class such that this separation (the so called \emph{margin}) is as large as possible. To find $\vec{\omega}$, a dual optimization problem has to be solved, to find for each document $d_j$ a value $\alpha_j$ such that the following holds:

\begin{equation*}
\vec{\omega} := \sum_j \alpha_j c_j \vec{d}_j,\quad \alpha_j \geq 0,
\end{equation*}

where $c_j \in \lbrace 1, -1\rbrace$ (in sentiment classification $1$ would denote "positive" and correspondingly $-1$ stands for "negative" sentiment). The actual classification of document $d$ is done, by determining on which side of $\vec{omega}$ its document vector $\vec{d}$ falls on.

\subsubsection*{Further techniques}

TODO