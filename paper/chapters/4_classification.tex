\section{Classification}


%Ideen:
%\begin{enumerate}
%\item Supervised vs. unsupervised
%\item Training Set
%\item FeatureVector
%\item N-Grams
%\end{enumerate}
%

Sentiment analysis of twitter data, or more general, the idea of extracting sentiment and opinions from pieces of text is based on the more general principle of \emph{classification} \cite{Pang2002}. The broad aim of classification is to assign given textual units to a set of classes or categories or the apply some kind of regression or ranking. Sentiment analysis is a specialization of this approach which aims at assigning sentiment values to documents. One application might involve to classify an opinionated text by assigning one of two opposing sentiment polarities, i.e. classifying it as either \emph{positive} or \emph{negative}. This classification task is also referred to as \emph{binary classification} or \emph{sentiment polarity classification} \cite{Pang2002}. But in general, the input to the sentiment classification process is not strictly opinionated which makes this task challenging. Therefore this kind of binary classification might not always be applicable. Different approaches that allow for a more fine grained classification might be appropriate, for example, based on a multi-point scale that allows for more than just two sentiment classes.

The characteristics of twitter messages introduce further challenges. Due to the structure of these messages, sentiment analysis of tweets is different from analyzing conventional texts, such as review documents, in various ways. The length of at most 140 characters and the rather informal spelling style pose problems that have to be considered carefully when analyzing the data. When preprocessing the texts these aspects already need to be handled so that the actual classification process is supplied with information that is considered useful for the analysis phase. After this step it is necessary to collect the relevant information as so called \emph{features} and organize them into a \emph{feature vector}.

\subsection{Features}
As twitter messages are composed of an arbitrary number of words makes it necessary to extract exactly those words which preferably describes the characteristics of such tweets. Extracting those words to an object by making use of various word selecting techniques is called a feature vector. These word selecting techniques are a fundamental process for providing informations to the classification methods, thus an extensive body of different word selecting algorithms exist. 
	\emph{Term Presence} is a method where the feature vector consists of a binary value, $0$ for "term is not present" and $1$ for "term is present". A similar approach is to count the number of occurred words in a message and use the numeric value as a feature, which is called \emph{Term Frequency} but Pang et al. showed \autocite{Pang2008} that Term Presence performed better compared to Term Frequency.
TODO

\subsection{Methods}

For the actual classification process extent literature distinguishes two general types of techniques, namely \emph{machine learning} and \emph{semantic orientation} \cite{Ye20096527}. The machine learning approach is also referred to as \emph{supervised learning}, since corresponding techniques generally require supervised training phases. Accordingly, semantic orientation is also called \emph{unsupervised learning} because such a training phase is not necessary for the process to work. The most prominent machine learning methods comprise \emph{support vector machines} and the \emph{Na\"ive Bayes} classifier.

\subsubsection*{Na\"ive Bayes}

This technique is based on a simple stochastic model that for a given document $d$ tries to find the most likely class $c^* = \mathrm{arg max}_c P(c \vert d)$ where $c$ stands for one out of a number of possible classes and $P(c \vert d)$ denotes the conditional probability that document $d$ can be assigned to $c$. For example, in sentiment analysis $c^*$ would denote the sentiment value that could be assigned to document $d$. $c^*$ is computed by applying Bayes' rule,

\begin{equation*}
P(c \vert d) = \frac{P(c)P(d \vert c)}{P(d)}
\end{equation*}

for each possible sentiment value $c$. $P(d \vert c)$ has to be estimated and therefore it is presumed that there exists a predefined set of features $\lbrace f_1, \ldots , f_m \rbrace$ which are assumed to be conditionally independent. Based on this estimation the actual rule that can be used for classification is derived from Bayes' rule:

\begin{equation*}
P_{\mathrm{NB}}(c \vert d) := \frac{P(c)(\prod^{m}_{i=1}P(f_i \vert c)^{n_i(d)})}{P(d)}
\end{equation*}

Here, $n_i(d)$ designates the number of occurrences of feature $f_i$ in document $d$.

\subsubsection*{Maximum Entropy}

\begin{equation*}
P_{\mathrm{ME}}(c \vert d) := \frac{1}{Z(d)}\mathrm{exp}\left( \sum_{i} \lambda_{i,c} F_{i,c}(d,c) \right)
\end{equation*}

\subsubsection*{Support Vector Machines}

\begin{equation*}
\vec{\omega} := \sum_j \alpha_j c_j \vec{d}_j,\quad \alpha_j \geq 0
\end{equation*}

\subsubsection*{Further techniques}

TODO