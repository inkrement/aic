\section{Preprocessing}
The data preprocessing step is an essential 
part of sentiment analysis. Its goal is to prepare data
for the sentiment analysis and remove noisy, unnecessary, 
inconsistent or incomplete parts.\autocite{Hemalatha2012}
\autocite{hemalatha2012preprocessing} In a first step a
tokenizer splits the tweet into conjugate parts, called words. 
The resulting list of words is filtered regarding the word class, 
the word itself and the intention of the sentiment analysis. 
In case of sentiment analysis of twitter posts it is common to 
remove the parts described in the following paragraphs.
\autocite{Hemalatha2012}

\paragraph{URLs}
Uniform Resource Locators (URLs) refer to another site, which 
in fact can contain very important information, but the url 
itself is not of interest, because in the most cases it 
contains autogenerated information, which can not be influenced 
by the poster.

\paragraph{Unnecesarry words}
It is not a ovious task to classify if a word is unnecesarry 
or not. But some word classes, like pronouns, articles orinterrogatives
do not contain any relevant information and can be skipped. (TODO:Referenz)

\paragraph{Retweets}
Retweets reflect the postion of another person and are not always
created to express the same position (it can also be a statement of sarcasm).

\paragraph{Repeated characters}
Everybody can post on Twitter, it is not limited 
to professional writers, authors or similar expert groups.
Therefore the tweets contain often deformed words 
containing repeated characters like `heeeey' or
`whuhuuu'. In this cases it is intended to reconstruct 
the correct english term and only if it is not 
possible or to sophisticated, the word is removed 
by the preprocessor.



\subsection{Tokenizer}
There are already free tokenizer implementations
 available. Some popular programming 
libraries for the english language include the Stanford 
NLP PTBTokenizer\footnote{http://nlp.stanford.edu/software/tokenizer.shtml} 
and the Apache OpenNLP Tokenizer\footnote{http://opennlp.apache.org/documentation/manual/opennlp.html}.
These examples are quite versatile and can be even 
used for different tasks and languages. Another 
example is the specialized Carnegie Mellon Twokenizer\footnote{http://www.ark.cs.cmu.edu/TweetNLP/},
which is especially for the tokenization of tweets.


It is a sophisticated task to recognize smileys. Some tokenizers.

%Besondere an twitter: Hashtags, Mentions, Smileys
Some words are difficult to handle. One example are emoticons,
because they are hard to recognize correctly. Although they 
are a good and popular ressource for sentiment analysis.


\subsection{Tagger}
