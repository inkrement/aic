\section{Preprocessing}
The data preprocessing step is an essential 
part of sentiment analysis. Its goal is to prepare data
for the sentiment analysis and remove noisy, unnecessary, 
inconsistent or incomplete parts.\autocite{Hemalatha2012}
In a first step a tokenizer splits the tweet into conjugate parts, called words. 
The resulting list of words is filtered regarding the word class, 
the word itself and the intention of the sentiment analysis. 
In case of sentiment analysis of twitter posts it is common to 
remove the parts described in the following paragraphs.\autocite{Hemalatha2012}

\paragraph{URLs}
Uniform Resource Locators (URLs) refer to another site, which 
in fact can contain very important information, but the internet adress 
on its own is not of interest, because in the most cases it 
contains only auto-generated information, which can not be influenced 
by the poster.

\paragraph{Unnecesarry words}
It is not a ovious task to classify if a word is unnecesarry 
or not. But some word classes, like pronouns, articles or interrogatives
do not contain any relevant information and can be skipped. (TODO:Referenz)

\paragraph{Retweets}
Retweets reflect the postion of another person and are not always
created to express the same position (e.g. statement of sarcasm).

\paragraph{Repeated characters}
Tweets contain often deformed words 
containing repeated characters like `heeeey' or
`whuhuuu'. In this cases it is intended to reconstruct 
the correct english term and only if it is not 
possible or to sophisticated, the word is removed 
by the preprocessor.


\subsection{Tokenizer}
There are already several open source tokenizer implementations
available. Some popular programming libraries for the english 
language include the Stanford NLP PTBTokenizer
\footnote{http://nlp.stanford.edu/software/tokenizer.shtml} 
and the Apache OpenNLP Tokenizer\footnote{http://opennlp.apache.org/documentation/manual/opennlp.html}.
These examples are quite versatile and can be even 
used for different tasks and languages.
To overcome the special needs of sentiment analysis in terms of Twitter posts,
highly specialized Tokenizer were created (e.g. Carnegie 
Mellon Twokenizer\footnote{http://www.ark.cs.cmu.edu/TweetNLP/}).
Some tokens like emoticons are difficult to handle. Although they 
are a good and popular ressource for sentiment analysis. \autocite{emoticons}
It is a sophisticated task to recognize them correctly and some Tokenizers 
like the Stanford NLP PTBTokenizer even skips them per default. 
The Carnegie Mellon's Twokenizer recognizes the related parts and
forwords them to the Tagger.

\subsection{Tagger}
